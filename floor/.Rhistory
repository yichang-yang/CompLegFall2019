}
for(i in 1 : length(commitee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing")) }
}
for(i in 1 : length(commitee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))
}
}
for(i in 1 : length(commitee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))
}
}
length(commitee)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))
}
}
length(committee)
}
}
}
}
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing")))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing")))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing")))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste0(file, "missing")))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, 'missing')))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs")
%>% html_attr("href")
url <- str_c("https://www.ourcommons.ca",scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, 'missing')))
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, 'missing'))) }
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))) }
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
tryCatch(url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))) }
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
tryCatch(url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap, file, quiet = TRUE)
error = function(e) print(paste(file, "missing"))) }
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url), error = function(e) print(paste, url, "missing")
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))) }
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap, file, quiet = TRUE), error = function(e) print(paste(file, "missing"))) }
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap, file, quiet = TRUE)}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.cascrap", scrap)
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.cascrap", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speechesDF <- a + b
a <- "House is burning"
b <- "Houses are gone"
speechesDF <- a + b
setwd("/Users/mark_alfie/Desktop/QTM\ 499R")
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
View(DTM)
a <- DTM[1,]
b <- DTM[2,]
distinct <- function(vec1, vec2){
for(i in 1:ncol(vec1) {
summation <- square(vect[i] - vect2[i])
}
return(sqrt(summation)
}
}
distinct <- function(vec1, vec2){
for(i in 1:ncol(vec1) {
summation <- (vect1 - vect2)^2
}
return(sqrt(summation)
}
distinct <- function(vec1, vec2){
summation <- (vect1 - vect2)^2
return(sqrt(summation)
}
function(vec1, vec2){
summation <- (vect1 - vect2)^2
return(sqrt(summation)
}
vect1 %*% vec2
distinct2 <- function(vec1, vec2) {
summation <- vect1 %*% vec2
}
distinct <- function(vec1, vec2){
summation <- (vect1 - vect2)^2
return(sqrt(summation))
}
distinct(a, b)
distinct <- function(vec1, vec2){
summation <- sum((vec1 - vec2)^2)
return(sqrt(summation))
}
distinct(a, b)
distinct2 <- function(vec1, vec2) {
summation <- sum(vec1 %*% vec2)
another <- sqrt(sum(vec1^2) + sum(vec2^2))
return (summation / another )
}
distinct2(a,b)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
library(XML)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(methods)
library(data.table)
library(stringr)
setwd("/Users/mark_alfie/Desktop/QTM\ 499R/floor")
committee <- c("ACVA", "AGRI","CHPC", "CIIT","CIMM", "ENVI","ERRE","ESPE","ETHI","FAAE","FEWO", "FINA","FOPO","HESA")
num <- c(123, 149, 163, 151, 167, 164, 57, 12, 160, 151, 149, 223, 152, 154)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
?read_html()
library(XML)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
library(xml2)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
library(revest)
library(rvest)
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
download.file(scrap1, file, quiet = TRUE)}
}
for(i in 1 : length(committee)) {
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",j,"/evidence", collapse = "")
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap1, file, quiet = TRUE),
error = function(e) print(paste(file, 'question missing')))
}
}
url
read_html(url)
htm <- read_html(url)
htm
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",2,"/evidence", collapse = "")
htm <- read_html(url)
htm
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",3,"/evidence", collapse = "")
read_html(url)
as.boolean(read.html("https://www.ourcommons.ca/DocumentViewer/en/42-1/ACVA/meeting-3/evidence"))
as.boolean(read.html("https://www.ourcommons.ca/DocumentViewer/en/42-1/ACVA/meeting-3/evidence"))
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",3,"/evidence", collapse = "")
if(error != read_html(url) ) {
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap1, file, quiet = TRUE),
error = function(e) print(paste(file, 'question missing')))
}
}
}
for(i in 1 : length(committee)) {
for(j in 1 : num[i]){
url <- str_c("https://www.ourcommons.ca/DocumentViewer/en/42-1/", committee[i], "/meeting-",3,"/evidence", collapse = "")
tryCatch(read_html(url), error = function(e) {print("NO")})
htm <- read_html(url)
scrap <- htm %>% html_nodes("btn.hidden-xs") %>% html_attr("href")
scrap1 <- str_c("https://www.ourcommons.ca", scrap, collapse = "")
file <- str_c("42-parliament-1-session", committee[i], "-meeting", j, ".xml", collapse = "")
tryCatch(download.file(scrap1, file, quiet = TRUE),
error = function(e) print(paste(file, 'question missing')))
}
}
}
# Initialize and load dataset
remove(list = ls())
install.packages('wooldridge')
library(wooldridge)
data(rental)
rental <- subset(rental, year == 90)
reg <- lm(lrent ~ lpop + lavginc + pctstu, data = rental)
(sum_reg <- summary(reg))
# Set significance level
alpha <- 0.01
# Critical value
df <- sum_reg$df[2]
alpha
cv <- qt(p = alpha/2, df = df, lower.tail = FALSE)
sum_reg
(reject1 <- (abs(tstat) > cv))
tstat <- coef(sum_reg)[4, 3]
(reject1 <- (abs(tstat) > cv))
# Chapter 4, Ex. C1 -----------------------------------------------------------
data(vote1)
vote1$lratio <- log(vote1$expendA/vote1$expendB)
reg2 <- lm(voteA ~ lratio + lexpendB + prtystrA, data = vote1)
summary(reg2)
reg <- lm(voteA ~ lexpendA + lexpendB + prtystrA, data = vote1)
summary(reg)
summary(reg2)
vote1$lratio <- log(vote1$expendA/vote1$expendB)
reg2 <- lm(voteA ~ lratio + lexpendB + prtystrA, data = vote1)
summary(reg2)
reg <- lm(voteA ~ lexpendA + lexpendB + prtystrA, data = vote1)
summary(reg)
data(rental)
rental <- subset(rental, year == 90)
reg <- lm(lrent ~ lpop + lavginc + pctstu, data = rental)
(sum_reg <- summary(reg))
