# these datasets come from http://www.data.parliament.uk/dataset
# they include:
# (1) Bills
# (2) Constituencies
# (3) Divisions
# (4) Written Answers
# (5) Written Questions
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
########################
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
# load speeches data from 38th parliament
speechesDF <- read.csv("data/canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
getwd()
# load speeches data from 38th parliament
speechesDF <- read.csv("../canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
View(speechesDF)
View(speechesDF)
speech_words <- speechesDF() %>%
unnest_tokens(word, text) %>%
count(observation_path, word, sort = TRUE)
# these datasets come from http://www.data.parliament.uk/dataset
# they include:
# (1) Bills
# (2) Constituencies
# (3) Divisions
# (4) Written Answers
# (5) Written Questions
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidytext", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# working directoy
setwd("~/Documents/GitHub/CompLegFall2019/data/uk_lower/")
# load speeches data from 38th parliament
speechesDF <- read.csv("../canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speech_words <- speechesDF() %>%
unnest_tokens(word, text) %>%
count(observation_path, word, sort = TRUE)
speech_words <- speechesDF %>%
unnest_tokens(word, text) %>%
count(observation_path, word, sort = TRUE)
speechesDF %>%
unnest_tokens(word, text)
?unnest_tokens
speech_words <- tibble(text=speechesDF$paragraph_text)
speech_words <- speech_words %>%
unnest_tokens(word, text)
speech_words <- tibble(text=speechesDF$paragraph_text)
speech_words <- speech_words %>%
unnest_tokens(word, text) %>%
count(observation_path, word, sort = TRUE)
speech_words <- speech_words %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE)
speech_words <- tibble(text=speechesDF$paragraph_text)
speech_words <- speech_words %>%
unnest_tokens(word, text) '
%>%
count(book, word, sort = TRUE)
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
###################################
# create function to clean speeches
# to create DTM
###################################
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
speech_words <- speech_words %>%
unnest_tokens(word, text)
speech_words <- tibble(text=speechesDF$paragraph_text)
speech_words <- speech_words %>%
unnest_tokens(word, text)
View(speech_words)
View(speech_words)
View(speechesDF)
speech_words <- tibble(text=speechesDF$paragraph_text)
View(speech_words)
View(speech_words)
library(dplyr)
library(janeaustenr)
austen_books()
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
count(speech_path, word, sort = TRUE)
# load speeches data from 38th parliament
speechesDF <- read.csv("../canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
count(speech_path, word)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
count("speech_path", word)
peechesDF %>%
unnest_tokens(word, paragraph_text)
speechesDF %>%
unnest_tokens(word, paragraph_text)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
count(speech_number, word)
str(speechesDF)
speechesDF$speech_number <- as.factor(speechesDF$speech_number)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
count(speech_number, word)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
speech_words <- speech_words %>%
count(speech_number, word)
# load speeches data from 38th parliament
speechesDF <- read.csv("../canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speechesDF$speech_number <- as.factor(speechesDF$speech_number)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>%
speech_words <- speech_words %>%
count(speech_number, word)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text)
speech_words <- speech_words %>%
count(speech_number, word)
View(speech_words)
?count
speech_words %>% group_by(speech_number) %>% count(word)
speech_words <- speech_words %>% group_by(speech_number) %>% count(word)
# load speeches data from 38th parliament
speechesDF <- read.csv("../canadaTextParsing/Examples/canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>% group_by(speech_number) %>% count(word)
speechesDF %>%
unnest_tokens(word, paragraph_text)
speechesDF %>%
unnest_tokens(word, paragraph_text) %>% group_by(speech_number)
speechesDF %>%
unnest_tokens(word, paragraph_text) %>% group_by(speech_number) %>% count(word)
speech_words <- speechesDF %>%
unnest_tokens(word, paragraph_text) %>% group_by(speech_number)
speech_words
View(speech_words)
str(speech_words)
speech_words %>% count(word)
