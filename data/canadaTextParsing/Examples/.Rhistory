<<<<<<< HEAD
para_num <- all_interv[j] %>% xml_find_all(".//ParaText")
for(p in 1:length(para_num)) {
para_id <- para_num[p] %>% xml_attr("id")
para_text <- para_num[p] %>% xml_text()
new_row <- data.frame(speech_type = type, speech_ID = spd_id, member_name = who,
paragraph_text = para_text, paratext_ID = para_id ,speech_date = date, word_count = NA, speech_number = j,
paragraph_number = p , parliament_number = 42, session_number = 1, chamber_number = 1 )
result <- rbind(result, new_row)
}
}
},error = function(e) {message(e)}, warning = function(se) {message(se)})
}
}
result <- data.frame(matrix(ncol = 13, nrow = 0))
x <- c("committee","speech_type", "speech_ID", "member_name", "paragraph_text", "paratext_ID" ,"speech_date", "word_count", "speech_number", "paragraph_number", "parliament_number", "session_number", "chamber_number")
colnames(result) <- x
for(a in 1:14 ) {
for(b in 1:num[1]){
tryCatch( {
file <- paste0("42-parliament-1-session-", committee[a], "-meeting-", b, ".xml")
data <- read_xml(file,useInternalNode=TRUE)
general <- xml_find_all(data, ".//ExtractedInformation//ExtractedItem")
date <- general[11] %>% xml_text()
all_interv <- data %>% xml_find_all(".//Intervention")
for(j in 1:length(all_interv)) {
type <- all_interv[j] %>% xml_attr("Type")
spd_id <- all_interv[j] %>% xml_attr("id")
who <- all_interv[j] %>% xml_find_first(".//Affiliation") %>% xml_text()
para_num <- all_interv[j] %>% xml_find_all(".//ParaText")
for(p in 1:length(para_num)) {
para_id <- para_num[p] %>% xml_attr("id")
para_text <- para_num[p] %>% xml_text()
new_row <- data.frame(committee = committee[a], speech_type = type, speech_ID = spd_id, member_name = who,
paragraph_text = para_text, paratext_ID = para_id ,speech_date = date, word_count = NA, speech_number = j,
paragraph_number = p , parliament_number = 42, session_number = 1, chamber_number = 1 )
result <- rbind(result, new_row)
}
}
},error = function(e) {message(e)}, warning = function(se) {message(se)})
}
}
View(result)
result <- data.frame(matrix(ncol = 14, nrow = 0))
x <- c("committee","meeting","speech_type", "speech_ID", "member_name", "paragraph_text", "paratext_ID" ,"speech_date", "word_count", "speech_number", "paragraph_number", "parliament_number", "session_number", "chamber_number")
colnames(result) <- x
for(a in 1:14 ) {
for(b in 1:num[1]){
tryCatch( {
file <- paste0("42-parliament-1-session-", committee[a], "-meeting-", b, ".xml")
data <- read_xml(file,useInternalNode=TRUE)
general <- xml_find_all(data, ".//ExtractedInformation//ExtractedItem")
date <- general[11] %>% xml_text()
all_interv <- data %>% xml_find_all(".//Intervention")
for(j in 1:length(all_interv)) {
type <- all_interv[j] %>% xml_attr("Type")
spd_id <- all_interv[j] %>% xml_attr("id")
who <- all_interv[j] %>% xml_find_first(".//Affiliation") %>% xml_text()
para_num <- all_interv[j] %>% xml_find_all(".//ParaText")
for(p in 1:length(para_num)) {
para_id <- para_num[p] %>% xml_attr("id")
para_text <- para_num[p] %>% xml_text()
new_row <- data.frame(committee = committee[a], meeting = b, speech_type = type, speech_ID = spd_id, member_name = who,
paragraph_text = para_text, paratext_ID = para_id ,speech_date = date, word_count = NA, speech_number = j,
paragraph_number = p, parliament_number = 42, session_number = 1, chamber_number = 1 )
result <- rbind(result, new_row)
}
}
},error = function(e) {message(e)}, warning = function(se) {message(se)})
}
}
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
speechesDF
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
=======
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
<<<<<<< HEAD
detachAllPackages
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
=======
detachAllPackages()
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
# load libraries
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
<<<<<<< HEAD
lapply(c("quanteda", "stringr", "tm"), pkgTest)
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
=======
lapply(c("quanteda", "stringr"), pkgTest)
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
<<<<<<< HEAD
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
library(tm)
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
install.packages("tm")
library(tm)
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
# check to see what it looks like
inspect(alternative_DTM[1:50, 1:50])
alternative_DTM
View(DTM)
DTM1 <- DTM[1:2,]
DTM1
View(DTM1)
N <- rows(DTM)
N <- nrows(DTM)
N <- nrow(DTM)
N <- nrow(DTM)
n_col <- ncol(DTM)
n <- subset(DTM, DTM[1] != 0)
n
View(n)
n <- subset(DTM, DTM[,1] != 0)
n
View(n)
occurrance <- length(n)
<- length(n)
occurrance
occurrance <- nrow(n)
occurrance
result <- c()
N <- nrow(DTM)
n_col <- ncol(DTM)
result <- c()
for(i in 1:n_col) {
n <- subset(DTM, DTM[,1] != 0)
occurrance <- nrow(n)
IDF <- log(N/occurrance)
result <- c(result, IDF)
}
result
ncol(DTM)
ncol(DTM)
N <- nrow(DTM)
n_col <- ncol(DTM)
result <- c()
for(i in 1:n_col) {
n <- subset(DTM, DTM[,i] != 0)
occurrance <- nrow(n)
IDF <- log(N/occurrance)
result <- c(result, IDF)
}
result
DTM*result
mark <- DTM*result
mark
View(mark)
result*DTM
mark <- result*DTM
result
?croosprod
?crossprod
mark <- result %*% DTM
mark <- DTM %*% result
mark
View(mark)
N
result
mark <- DTM %*% result
mark
n_col <- ncol(DTM)
result <- c()
freq <- colsums(DTM)
freq <- colSums(DTM)
IDF <- log(94/freq)
IDF
freq
freq <- colSums(DTM)
freq
idf <-diag[IDF]
idf
tf <- as.matrix(t(DTM))
tf <- as.matrix(t(DTM))
idf <- log(ncol(tf)/ rowSums(tf))
idf <- diag(idf)
idf
View(idf)
for(i in 1:n_col) {
n <- subset(DTM, DTM[,i] != 0)
occurrance <- rowSums(n)
IDF <- log(N/occurrance)
result <- c(result, IDF)
}
result
idf <- diag(result)
tf_idf <- tf %*% idf
colnames(tf_idf) <- rownames(tf)
tf_idf <- tf %*% idf
colnames(tf_idf) <- rownames(tf)
idf <- diag(result)
tf_idf <- tf %*% idf
tf_idf <- idf %*% tf
idf <- diag(result)
tf_idf <- idf %*% tf
dim(idf)
tf <- as.matrix(t(DTM))
idf <- log(ncol(tf)/ rowSums(tf))
idf <- diag(result)
tf_idf <- idf %*% tf
dim(tf)
idf <- log(ncol(tf)/ rowSums(tf))
idf
idf <- diag(idf)
tf_idf <- idf %*% tf
colnames(tf_idf) <- rownames(tf)
colnames(tf_idf) <- rownames(tf)
colnames(tf_idf) <- colnames(tf)
tf_idf
View(tf_idf)
tf <- as.matrix(t(DTM))
idf <- log(ncol(tf)/ rowSums(tf))
idf <- diag(idf)
idf
View(idf)
clear
list = list(remove())
remove(list=ls())
knitr::opts_chunk$set(echo = TRUE)
remove(list=ls())
library(wooldridge)
wage <-  wooldridge::wage2
names(wage)
reg_r <- lm(lwage ~ educ + exper + tenure + married + black + south + urban, data = wage) # restricted
wage$exper_sq <- (wage$exper)^2
wage$tenure_sq <- (wage$tenure)^2
reg_un <- lm(lwage ~ educ + exper + tenure + married + black + south + urban + exper_sq + tenure_sq, data = wage) # Unrestricted
names(summary(reg_un))
F_stats <- ((summary(reg_un)$r.squared - summary(reg_r)$r.squared) / 2 ) / ( (1 - summary(reg_r)$r.squared) / 925)
pf(F_stats, 2, 925, lower.tail = F)
(reject <- F_stats > qf(0.2, 2, 925, lower.tail = F))
#allow the return to education to depend on race
reg_extend <- lm(lwage ~ educ + exper + tenure + married + black + south + urban + educ:black, data = wage)
summary(reg_extend)$coef[9,4] # the p-value for educ:black
wage1 <- wooldridge::wage1
wage1$educ <- wage1$educ - 12.5
reg3 <- lm(lwage ~ female + educ + female:educ + exper + expersq + tenure + tenursq, data = wage1)
summary(reg3)$coef[2,1] #female's coefficient
beauty <- wooldridge::beauty
female <- subset(beauty, female == 1)
reg4 <- lm(lwage ~ belavg + abvavg, data = female)
summary(reg4)$coef[2,3] #-1.81
# one-side t test
qt(0.05, 433, lower.tail = T) # -1.64
(reject2 <- summary(reg4)$coef[2,3] < qt(0.05, 433, lower.tail = T))
reg_in <- lm(lwage ~ belavg + abvavg + educ + exper + I(exper^2))
reg_in <- lm(lwage ~ belavg + abvavg + educ + exper + I(exper^2), data = beauty)
linearHypothesis(reg_in, c("belavg = 0", "abvavg = 0"), test = c("F"), white.adjust = "hc1")
?linearHypothesis(reg_in, c("belavg = 0", "abvavg = 0"), test = c("F"), white.adjust = "hc1")
library(car)
install.packages("car")
library(car)
knitr::opts_chunk$set(echo = TRUE)
library(car)
reg_in <- lm(lwage ~ belavg + abvavg + educ + exper + I(exper^2), data = beauty)
linearHypothesis(reg_in, c("belavg = 0", "abvavg = 0"), test = c("F"), white.adjust = "hc1")
white_test <- lm(resid_sq ~ belavg + abvavg + educ + exper + I(exper^2), data = beauty)
beauty$fitted <- reg_in$fitted.values
white_test <- lm(resid_sq ~ fitted + fitted^2, data = beauty)
beauty$resid_sq <- (reg_in$residuals)^2
white_test <- lm(resid_sq ~ fitted + fitted^2, data = beauty)
summary(white_test)
white_test <- lm(resid_sq ~ fitted + I(fitted^2), data = beauty)
summary(white_test)
beauty$fitted <- reg_in$fitted.values
beauty$resid_sq <- (reg_in$residuals)^2
white_test <- lm(resid_sq ~ fitted + I(fitted^2), data = beauty)
summary(white_test)
beauty$fitted <- reg_in$fitted.values
beauty$resid_sq <- (reg_in$residuals)^2
white_test <- lm(resid_sq ~ fitted + I(fitted^2), data = beauty)
summary(white_test)
reg_in$fitted.values
qf(0.2, 2, 925, lower.tail = F)
pf(F_stats, 2, 925, lower.tail = F)
qf(0.2, 2, 925, lower.tail = F)
white_test <- lm(resid_sq ~ fitted + fitted^2, data = beauty)
summary(white_test)
qf(0.2, 2, 1000, lower.tail = F)
(reject <- F_stats > qf(0.2, 2, 925, lower.tail = F))
white_test <- lm(resid_sq ~ fitted + I(fitted^2), data = beauty)
summary(white_test)
F_stats
pf(F_stats, 2, 925, lower.tail = F)
reg4 <- lm(lwage ~ belavg + abvavg  + educ + exper + exper2 + union + goodhlth + black + married + south + bigcity + smllcity + service, data = female)
reg4 <- lm(lwage ~ belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + smllcity + service, data = female)
summary(reg4)$coef[2,3] #-1.81
summary(reg4)
reg4 <- lm(lwage ~ belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + smllcity + service, data = beauty)
reg4 <- lm(lwage ~ female + belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + smllcity + service, data = beauty)
reg4
bigcity
beauty$bigcity
beauty$smllcity
reg4_u <- lm(lwage ~ female + belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + service + female:belavg + female:abvavg + female:educ + female:exper + female:I(exper^2) + female:union + female:goodhlth + female:black + female:south + female:married + female:bigcity + female:service, data = beauty)
reg4_r$residul
reg4_r <- lm(lwage ~ female + belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + service, data = beauty)
sum(reg4_r$residuals^2)
ssr_u <- sum(reg4_u$residuals^2)
ssr_u
ssr_r
ssr_r <- sum(reg4_r$residuals^2)
ssr_r
summary(reg4_r)$df
summary(reg4_u)$df
summary(reg4_u)$df
f_stats <- (ssr_r - ssr_u) / summary(reg4_r)$df
summary(reg4_r)$df
summary(reg4_r)$df - summary(reg4_u)$df
summary(reg4_u)$df - summary(reg4_r)$df
summary(reg4_u)$df
summary(reg4_r)$df
summary(reg4_r)$df
reg4_r <- lm(lwage ~ female + belavg + abvavg  + educ + exper + I(exper^2) + union + goodhlth + black + married + south + bigcity + service, data = beauty)
summary(reg4_r)$df
summary(reg4_r)$df
summary(reg4_u)$df
summary(reg4_r)$df
summary(reg4_u)$df
f_stats <- ((ssr_r - ssr_u) / summary(reg4_u)$df - summary(reg4_r)$df) / ((ssr_u) / summary(reg4_r)$df)
summary(reg4_r)$df
(summary(reg4_u)$df - summary(reg4_r)$df)
y
f_stats <- ((ssr_r - ssr_u) / (summary(reg4_u)$df - summary(reg4_r)$df)) / ((ssr_u) / summary(reg4_r)$df)
f_stats
summary(reg4_r)$df[1]
f_stats <- ((ssr_r - ssr_u) / (summary(reg4_r)$df[2] - summary(reg4_u)$df[2])) / ((ssr_u) / summary(reg4_r)$df[2])
# Test statistic
qf(0.05, 12, 1246, lower.tail = T) # -1.64
f_stats
# F Test statistic = 0.4346713
pf(4.356, 12, 1246, lower.tail = T)
# F Test statistic = 0.4346713
pf(4.356, 12, 1246)
# F Test statistic = 0.4346713
pf(4.356, 12, 1246, lower.tail = F)
(reject2 <- f_stats > 0.05)
# F Test statistic = 0.4346713
pf(4.356, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(50, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(0, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
qf(1.64, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
qf(1.64, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(1.64, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(0.05, 12, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(0.05, 100000, 1246, lower.tail = F)
# Probability of F Test statistic getting more extreme than 7.885017e-07
pf(4.356, 12, 1246, lower.tail = F)
(reject2 <- f_stats > 0.05)
=======
DTM[50,]
tabel9which(DTM[50,]>0)
table(which(DTM[50,]>0)
)
View(all_words)
>>>>>>> 3c41c2b545e982c5916d84e182495035bd5d7a75
