nd load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
# create list of basic libraries that are essential
basic.packages <- c("package:stats","package:graphics","package:grDevices",
"package:utils","package:datasets","package:methods","package:base")
# find all packages that are floating in the global environment
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
# make sure that basic packages aren't dropped
package.list <- setdiff(package.list,basic.packages)
# remove all extra packages
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
# execute function to keep only basic packages in global envir.
detachAllPackages()
# set working directory
# which should be the same for everyone
# assuming GitHub is in your "Documents" folder
setwd("~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Members (CSV)/")
# load libraries
pkgTest <- function(pkg){
# check whether a package is installed
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
# if there are any packeages that need to be installed
if (length(new.pkg))
# install package
install.packages(new.pkg, dependencies = TRUE)
# and then load all packages user listed
sapply(pkg, require, character.only = TRUE)
}
# apply function to load libraries and install them if necessary
lapply(c("stringr", "plyr", "tidyverse", "tidyr", "dplyr"), pkgTest)
#######################
# load dataset of MPs
#######################
# read in your .csv files
# first, check which files are in the working directory
filenames <- gsub("\\.csv$","", list.files(pattern="\\.csv$"))
# iterate over those file names and read each .csv
for(i in filenames){
# Tip: you almost always want to read stringsAsFactors=F in .csv
# because you want to often text as text, not factors
assign(i, read.csv(paste(i, ".csv", sep=""), stringsAsFactors = F, encoding = "UTF-8"))
}
# URL address
url <- "https://en.wikipedia.org/wiki/House_of_Commons_of_Canada"
# download file to working directory
table <- readLines(url)
# collapse vector of HTML code to a single string
table <- str_c(table, collapse = " ")
# extract tables
table <- readHTMLTable(table)
# apply function to load libraries and install them if necessary
lapply(c("stringr", "rvest", "XML", "plyr", "tidyverse", "tidyr", "dplyr"), pkgTest)
# URL address
url <- "https://en.wikipedia.org/wiki/House_of_Commons_of_Canada"
# download file to working directory
table <- readLines(url)
# collapse vector of HTML code to a single string
table <- str_c(table, collapse = " ")
# extract tables
table <- readHTMLTable(table)
# get a summary of the object
summary(table)
# download file to working directory
table <- readLines(url)
table
# download file to working directory
table <- readLines(url)
# collapse vector of HTML code to a single string
table <- str_c(table, collapse = " ")
# extract tables
table1 <- readHTMLTable(table)
# get a summary of the object
summary(table)
table
table1
# URL address
url <- "https://en.wikipedia.org/wiki/Canadian_federal_electoral_redistribution,_2012"
# download file to working directory
table <- readLines(url)
# collapse vector of HTML code to a single string
table <- str_c(table, collapse = " ")
# extract tables
table1 <- readHTMLTable(table)
# we want the 6th element in the list
table <- table[[5]]
# URL address
url <- "https://en.wikipedia.org/wiki/Canadian_federal_electoral_redistribution,_2012"
# download file to global environment
table <- readLines(url)
# collapse vector of HTML to single string
table <- str_c(table, collapse = " ")
# extract tables
table <- readHTMLTable(table)
table[[1]]
# replace variable names
names(table) <- c("province_territory", "current_seats", "projected_seats_1985formula", "projected_seats_Newformula")
# remove the first row
table <- table[-1,]
# URL address
url <- "https://en.wikipedia.org/wiki/Canadian_federal_electoral_redistribution,_2012"
# download file to global environment
table <- readLines(url)
# collapse vector of HTML to single string
table <- str_c(table, collapse = " ")
# extract tables
table <- readHTMLTable(table)
# we want the 6th element in the list
table<- table[[1]]
# view variable names
names(table)
# replace variable names
names(table) <- c("province_territory", "current_seats", "projected_seats_1985formula", "projected_seats_Newformula")
# remove the first row
table <- table[-1,]
View(table)
# download file to global environment
table <- readLines(url)
# collapse vector of HTML to single string
table <- str_c(table, collapse = " ")
# extract tables
table <- readHTMLTable(table)
# we want 1st element in list
table<- table[[1]]
# view variable names
names(table)
# replace variable names
names(table) <- c("province_territory", "current_seats",
"projected_seats_1985formula",
"projected_seats_Newformula")
View(table)
# remove the first two rows
table <- table[-1:2,]
# remove the first two rows
table <- table[2:,]
dim(table)
# remove the first two rows
table <- table[c(3:dim(table)[1]),]
str_trim(table$province_territory)
?str_trim
# replace single occurence
str_replace(text, "British Columbia", "BC")
# trim white space around string
text <- str_trim(table$province_territory)
# replace single occurence
str_replace(text, "British Columbia", "BC")
# replace multiple occurences
str_replace_all(text, "Old", "New")
# replace multiple occurences
str_replace_all(text, "Old", "New")
# detect expression
str_detect(text, "and")
